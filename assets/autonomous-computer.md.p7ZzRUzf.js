import{_ as t,c as e,o,a3 as a}from"./chunks/framework.BH-CNZX5.js";const s="/assets/labeled_copy.Dx_YlO7r.png",g=JSON.parse('{"title":"Autonomous Computers","description":"","frontmatter":{},"headers":[],"relativePath":"autonomous-computer.md","filePath":"autonomous-computer.md"}'),n={name:"autonomous-computer.md"},r=a('<h1 id="autonomous-computers" tabindex="-1">Autonomous Computers <a class="header-anchor" href="#autonomous-computers" aria-label="Permalink to &quot;Autonomous Computers&quot;">â€‹</a></h1><p>Date: February 3, 2024</p><p>There&#39;s a lot of interest in autonomously operating GUI&#39;s with multi-modal models (rabbit r1, and next gen Siri)</p><p>Sure, it would be more practical if you could do everything through the command line..</p><p>But in the end, you can&#39;t order Chipotle or file your taxes through the command line.</p><p>This being the case, I think there is a big opportunity for curating a GUI image segmentation dataset.</p><p>Current state-of-the-art vision and segmentation models- GPT4 vision, LLaVA, SAM, YOLO, etc, all struggle with these tasks (at least before fine-tuning)</p><p><img src="'+s+'" alt="Base SAM struggling to segment UI elements"></p><p>Base SAM struggling to segment UI elements</p><p>Surprisingly smaller OCR (optical character recognition) models, like EasyOCR, excel at this task; but not all UI elements contain text.</p><p>Data collection would be somewhat straightforward- screenshots with bounding boxes produced by a click listener ðŸ¤”</p>',11),i=[r];function u(l,m,p,c,d,h){return o(),e("div",null,i)}const f=t(n,[["render",u]]);export{g as __pageData,f as default};
