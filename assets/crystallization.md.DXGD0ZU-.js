import{_ as e,c as t,o as a,M as s}from"./chunks/framework.Dt16nE2a.js";const i="/assets/protein-shape-forces-bonds-self-assembly-folding-l.Cc9O9RXM.jpg",n="/assets/Screenshot_2024-01-19_at_9.22.41_PM.Cz0w8eDV.png",f=JSON.parse('{"title":"Language Modeling is Crystallization ❄️","description":"","frontmatter":{},"headers":[],"relativePath":"crystallization.md","filePath":"crystallization.md"}'),o={name:"crystallization.md"},r=s('<h1 id="language-modeling-is-crystallization-❄️" tabindex="-1">Language Modeling is Crystallization ❄️ <a class="header-anchor" href="#language-modeling-is-crystallization-❄️" aria-label="Permalink to &quot;Language Modeling is Crystallization ❄️&quot;">​</a></h1><p>Date: January 21, 2024</p><p>My chemistry background tempts me to make a <em>very</em> rough analogy between protein folding/ crystallization and the construction of language by LLMs</p><p>Both processes construct high level complexity from atomic units.</p><p>Attention and syntax in language synthesis is similar to the van der Waals force and bond angles in crystallization.</p><p>Local interactions give rise to complex emergent properties of the larger system.</p><p><img src="'+i+'" alt="Protein crystallization structure is guided by a multitude of intramolecular forces"></p><p>Protein crystallization structure is guided by a multitude of intramolecular forces</p><p><img src="'+n+'" alt="Transformer self-attention calculates relevance between tokens"></p><p>Transformer self-attention calculates relevance between tokens</p><p>In language, the emergent structures produced are sentence/ essay/ novel. In crystallization, it is snowflakes/ zeolites/ protein crystals.</p><p>In reality, language synthesis is a more stochastic process, whereas crystallization is governed entirely by deterministic physical forces.</p><p>Leaving the analogy aside, the fact that language is computationally reducible <em>at all</em>, is absolutely mind blowing. Especially considering that language is a construct created by humans, full of arbitrary syntactic and semantic conventions.</p><p>Yet somehow, there seem to be fundamental &quot;laws of language&quot; that enable it to be deterministically modeled by LLMs.</p><hr><blockquote><p><em>&quot;It&#39;s as if we&#39;re discovering a new science- the science of the emergent properties of LLMs.</em></p><p><em>For instance, LLM&#39;s have a temperature parameter that determines how tightly or loosely the network&#39;s output abides by it&#39;s statistical predictions. A low temperature results in a deterministic output. A high temperature results in incoherent language.</em></p><p><em>But there is a goldilocks temperature range that generative models like GPT-4 use, where the perfect balance of creativity and coherent language is achieved.</em></p><p><em>When the model&#39;s temperature is changed, it&#39;s as if the model goes through a phase change similar to matter.&quot;</em></p><p>-Stephen Wolfram (paraphrased)</p></blockquote>',16),l=[r];function c(p,m,u,g,d,h){return a(),t("div",null,l)}const _=e(o,[["render",c]]);export{f as __pageData,_ as default};
