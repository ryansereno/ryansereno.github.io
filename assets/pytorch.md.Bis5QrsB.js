import{_ as e,c as t,o as r,a3 as a}from"./chunks/framework.BH-CNZX5.js";const s="/assets/Screen-Shot-2017-07-16-at-4.44.08-PM.D8SZISlR.png",g=JSON.parse('{"title":"Understanding The Core of ML Libraries ∂L/∂z","description":"","frontmatter":{},"headers":[],"relativePath":"pytorch.md","filePath":"pytorch.md"}'),i={name:"pytorch.md"},n=a('<h1 id="understanding-the-core-of-ml-libraries-∂l-∂z" tabindex="-1">Understanding The Core of ML Libraries ∂L/∂z <a class="header-anchor" href="#understanding-the-core-of-ml-libraries-∂l-∂z" aria-label="Permalink to &quot;Understanding The Core of ML Libraries ∂L/∂z&quot;">​</a></h1><p>Date: June 24, 2023</p><p>This is a Work in Progress</p><p>ML libraries like TensorFlow and PyTorch can feel overwhelming</p><p>But their core purpose is simple.</p><p>They have three primary functions:</p><ul><li>Create/ handle tensors</li><li>Calculate gradients</li><li>Provide an optimizer (optional)</li></ul><p>Here’s we will focus on the calculating of gradients as it is the core mechanism through which neural nets learn.</p><p><img src="'+s+'" alt="Screen-Shot-2017-07-16-at-4.44.08-PM.png"></p><p>Neural network &#39;learning&#39; involves incrementally adjusting the model&#39;s parameters, based on the gradient of the loss function.</p><p>These adjustments are made in the direction that <em>minimizes</em> the loss function, a process called gradient descent.</p><p>To derive these gradients, all operations and their inputs during the forward pass must be tracked so that their derivatives can be calculated during the backward pass.</p><p>This process, known as backpropagation, uses the chain rule to efficiently compute these gradients.</p><p>… <em>Work in progress</em></p>',14),o=[n];function l(c,p,h,d,m,u){return r(),t("div",null,o)}const f=e(i,[["render",l]]);export{g as __pageData,f as default};
