import{_ as e,c as t,o as r,M as a}from"./chunks/framework.Dt16nE2a.js";const s="/assets/Screen-Shot-2017-07-16-at-4.44.08-PM.D8SZISlR.png",g=JSON.parse('{"title":"Understanding The Core of ML Libraries ∂L/∂z","description":"","frontmatter":{},"headers":[],"relativePath":"pytorch.md","filePath":"pytorch.md"}'),i={name:"pytorch.md"},n=a('<h1 id="understanding-the-core-of-ml-libraries-∂l-∂z" tabindex="-1">Understanding The Core of ML Libraries ∂L/∂z <a class="header-anchor" href="#understanding-the-core-of-ml-libraries-∂l-∂z" aria-label="Permalink to &quot;Understanding The Core of ML Libraries ∂L/∂z&quot;">​</a></h1><div class="tip custom-block"><p class="custom-block-title">This article is a work in progress</p></div><p>ML libraries like TensorFlow and PyTorch can feel overwhelming</p><p>But their core purpose is simple.</p><p>They have three primary functions:</p><ul><li>Create tensors and handle operations on them</li><li>Calculate gradients</li><li>Provide an optimizer (optional)</li></ul><p>Here’s we will focus on the calculating of gradients as it is the core mechanism through which neural nets learn.</p><p><img src="'+s+'" alt="Screen-Shot-2017-07-16-at-4.44.08-PM.png"></p><p>Neural network &#39;learning&#39; involves incrementally adjusting the model&#39;s parameters, based on the gradient of the loss function.</p><p>These adjustments are made in the direction that <em>minimizes</em> the loss function, a process called gradient descent.</p><p>To derive these gradients, all operations and their inputs during the forward pass must be tracked so that their derivatives can be calculated during the backward pass.</p><p>This process, known as backpropagation, uses the chain rule to efficiently compute these gradients.</p><p>… <em>Work in progress</em></p>',13),o=[n];function l(c,p,d,h,m,u){return r(),t("div",null,o)}const f=e(i,[["render",l]]);export{g as __pageData,f as default};
